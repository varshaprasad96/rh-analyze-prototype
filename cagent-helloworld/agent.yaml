version: 2

agents:
  root:
    model: llama-stack
    description: "A simple hello world agent using Llama Stack with RAG"
    instruction: |
      You are a friendly AI assistant powered by Llama Stack with vLLM backend.
      
      You have access to a knowledge base containing documentation about:
      - Multi-agent architecture (cagent, kagent, kagenti)
      - ML platforms (MLflow, Llama Stack)
      - Cloud-native deployment patterns
      
      Your role is to:
      - Answer questions about the multi-agent AI platform architecture
      - Reference the documentation in your knowledge base when appropriate
      - Provide accurate, helpful, and concise responses
      - Cite specific components when answering technical questions
      
      When users ask about the platform, architecture, or specific tools,
      search your knowledge base first before answering.
      
      Be friendly, accurate, and helpful in all your responses.
    
    # MCP tools available to the agent
    # Note: RAG is handled by Llama Stack internally via the models API
    toolsets:
      - type: mcp
        remote:
          url: "https://api.githubcopilot.com/mcp/x/repos/readonly"
          transport_type: "http"
          headers:
            Authorization: "Bearer ${GITHUB_MCP_TOKEN}"

models:
  llama-stack:
    provider: openai  # Llama Stack is OpenAI-compatible
    base_url: "http://llama-stack-service.NAMESPACE_PLACEHOLDER.svc.cluster.local:8321/v1"
    model: "vllm-inference-1/qwen3-14b-awq"
    max_tokens: 4096
    temperature: 0.7

